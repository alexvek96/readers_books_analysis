{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qe7BpQKS0Da"
      },
      "source": [
        "# **Data Mining Techniques (Ex 2)**\n",
        "# **| Readers and Books analysis, recommendation and classification system dev**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR4nW6vobH0D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import ast\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUqFzHsDf5Jc"
      },
      "outputs": [],
      "source": [
        "# Replace it with your path\n",
        "DIR_PATH = \"/content/gdrive/MyDrive/\"\n",
        "\n",
        "# path of the .csv file\n",
        "file_path = DIR_PATH + \"books_1.Best_Books_Ever.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEAnsJYkTpdJ"
      },
      "source": [
        "* Creating a dataframe from the .csv file.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVOtTGKBb5St"
      },
      "outputs": [],
      "source": [
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)    # Display all columns\n",
        "pd.set_option('display.max_colwidth', None)   # Display full column width\n",
        "pd.set_option('display.max_rows', None)       # Display all rows\n",
        "\n",
        "# Read .csv file into DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the first 50 rows of the resulting DataFrame, as an example, to avoid RAM crash\n",
        "df.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPqbzRDXaRcK"
      },
      "source": [
        "### Question 1: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFrymstBk4x3"
      },
      "source": [
        "* Creating 5 additional columns in the DataFrame, containing the separate values of the \"*ratingsByStars*\" column for each star/category (-> \"*ratingStar_1, ratingStar_2, ratingStar_3, ratingStar_4, ratingStar_5*\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSKtZutYaRkD"
      },
      "outputs": [],
      "source": [
        "# Split values in \"ratingsByStars\" column and assign to new columns \"ratingStar_i\", i=1...5\n",
        "split_columns = [f'ratingStar_{i+1}' for i in range(5)]\n",
        "ratings_df = pd.DataFrame(df['ratingsByStars'].str.split(',', expand=True), columns=split_columns)\n",
        "df1 = df.copy()\n",
        "df1 = pd.concat([df, ratings_df], axis=1)\n",
        "\n",
        "# Print the first 50 rows of the resulting DataFrame, as an example, to avoid RAM crash\n",
        "df1.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgU24oipnlBl"
      },
      "source": [
        "* Creating an additional column \"*genreSingle*\" where we store the first genre value from the original \"*genre*\" column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg-InPN-maVy"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "df2 = df.copy()\n",
        "\n",
        "# Remove rows with NaN values in the columns that we are going to process in the sub-question\n",
        "df2.dropna(subset=['genres'], inplace=True)\n",
        "\n",
        "# Extract the first element from the \"genres\" column and save it into a new column \"genreSingle\"\n",
        "df2['genreSingle'] = df2['genres'].str.split(',').str[0].str.strip().str.replace(r'\\[', '', regex=True)\n",
        "\n",
        "# Print the first 50 rows of the resulting DataFrame, as an example, to avoid RAM crash\n",
        "df2.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGdR5NNaoxgl"
      },
      "source": [
        "* Creating an additional column \"*publishDate*\" where we store the publication date of the book, based on the original \"*publishDate*\" column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T31S3SOZouzG"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "df3 = df.copy()\n",
        "\n",
        "# Remove rows with NaN values in the columns that we are going to process in the sub-question\n",
        "df3 = df3.dropna(subset=['publishDate'])\n",
        "\n",
        "# Convert \"publishDate\" column to datetime format\n",
        "df3['publishDate'] = pd.to_datetime(df3['publishDate'], errors='coerce')\n",
        "\n",
        "# Extract the year from \"publishDate\" and save it in \"PublishYear\" column\n",
        "df3.loc[:, 'PublishYear'] = df3['publishDate'].dt.year\n",
        "\n",
        "# Print the first 50 rows of the resulting DataFrame, as an example, to avoid RAM crash\n",
        "df3.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv6z6aEqdg-7"
      },
      "source": [
        "### Question 2: Extracting data quering the DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r5YzPSY4FQT"
      },
      "source": [
        "Before data processing in each sub-question, we create a new copy of the original dataframe. In this copy, we drop the rows for which there are NaN values in the corresponding columns we are processing. Otherwise, if we indiscriminately drop the rows for which there are NaN values in any column or the column we are processing, we encounter the following problem:\n",
        "When we reuse the same modified dataframe in a subsequent question, we may lose data rows from before, even though in our new question, the processing column would not have any NaN values!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrDID50Xu77V"
      },
      "source": [
        "2.   Find the 10 books with the most pages\n",
        "     (Ποια είναι τα 10 βιβλία με τις περισσότερες σελίδες)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilPLCyE2u8Pe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_2_2 = df.copy()\n",
        "\n",
        "# Remove rows with NaN values in the columns that we are going to process in the sub-question\n",
        "df_2_2.dropna(subset=['title', 'pages'], inplace=True)\n",
        "\n",
        "# Convert 'pages' column to numeric dtype\n",
        "df_2_2['pages'] = pd.to_numeric(df_2_2['pages'], errors='coerce')\n",
        "\n",
        "# Sort the dataframe by 'pages' column in descending order and select the top 10\n",
        "top_10_books = df_2_2.sort_values('pages', ascending=False).head(10)\n",
        "\n",
        "# Generate a color palette with a unique color for each book title\n",
        "colors = plt.cm.tab10(np.arange(len(top_10_books)))\n",
        "\n",
        "# Plot the histogram with colored bars\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.bar(top_10_books['title'], top_10_books['pages'], color=colors)\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Top 10 Books with Most Pages')\n",
        "plt.xlabel('Book Title')\n",
        "plt.ylabel('Number of Pages')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=8)     # Rotate and align x-axis labels\n",
        "plt.ticklabel_format(style='plain', axis='y')       # Display y-axis values in plain format\n",
        "\n",
        "# Add total number of pages above each bar\n",
        "for i, pages in enumerate(top_10_books['pages']):\n",
        "    plt.text(i, pages, int(pages), ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# Plotting\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6TVssIbtCvh"
      },
      "source": [
        "\n",
        "\n",
        "5.   Find the 10 writers with the most books in their writing history\n",
        "     (Ποιοι είναι οι 10 συγγραφεις με τα περισσότερα βιβλία).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2kPbljkv1g8"
      },
      "outputs": [],
      "source": [
        "df_2_5 = df.copy()\n",
        "\n",
        "# Remove rows with NaN values in the columns that we are going to process in the sub-question\n",
        "df_2_5.dropna(subset=['title', 'author'], inplace=True)\n",
        "\n",
        "# Group the dataframe by writer and count the number of books\n",
        "writer_counts = df_2_5['author'].value_counts().nlargest(10)\n",
        "\n",
        "# Generate a color palette with a unique color for each author\n",
        "colors = plt.cm.tab10(np.arange(len(writer_counts)))\n",
        "\n",
        "# Plot the histogram with colored bars\n",
        "plt.figure(figsize=(10, 6))\n",
        "writer_counts.plot(kind='bar', color=colors)\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Top 10 Authors with Most Books')\n",
        "plt.xlabel('Author')\n",
        "plt.ylabel('Number of Books')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=8)  # Rotate and align x-axis labels\n",
        "plt.yticks(fontsize=8)                           # Adjust y-axis tick font size\n",
        "\n",
        "# Add total book count above each bar for better visualisation\n",
        "for i, count in enumerate(writer_counts):\n",
        "    plt.text(i, count, count, ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# Plotting\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G7SLbN3dhC9"
      },
      "source": [
        "6.   Find the 10 writers with the most critics in their writing history\n",
        "     (Ποιοι είναι οι 10 συγγραφεις με τις περισσότερες κριτικές για τα βιβλία τους)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqz6XJOe2U-_"
      },
      "outputs": [],
      "source": [
        "df_2_6 = df.copy()\n",
        "\n",
        "# Remove rows with NaN values in the columns that we are going to process in the sub-question\n",
        "df_2_6.dropna(subset=['author', 'numRatings'], inplace=True)\n",
        "\n",
        "# Create a copy of the DataFrame\n",
        "df_copy = df_2_6.copy()\n",
        "\n",
        "# Split the author column by comma and extract the first part\n",
        "# We want only the name of the author, we must ignore the names of the editor, illustrator, etc. if any\n",
        "df_copy['author_name'] = df_copy['author'].str.split(',').str[0]\n",
        "\n",
        "# Group the dataframe by first name and sum the critics\n",
        "writer_total_critics = df_copy.groupby('author_name')['numRatings'].sum().nlargest(10)\n",
        "\n",
        "# Generate a color palette with a unique color for each author\n",
        "colors = plt.cm.tab10(np.arange(len(writer_total_critics)))\n",
        "\n",
        "# Plot the histogram with colored bars\n",
        "plt.figure(figsize=(10, 10))\n",
        "writer_total_critics.plot(kind='bar', color=colors)\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Top 10 Authors with Most Critics')\n",
        "plt.xlabel('Author')\n",
        "plt.ylabel('Number of Critics')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=8)     # Rotate and align x-axis labels\n",
        "plt.ticklabel_format(style='plain', axis='y')       # Display y-axis values in plain format\n",
        "\n",
        "# Add total number of critics on top of each bar\n",
        "for i, count in enumerate(writer_total_critics):\n",
        "    plt.text(i, count, count, ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# Plotting\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Delete the df copy, we used it only for data mining and plotting\n",
        "del df_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5pjoOEozp9M"
      },
      "source": [
        "8.   Find the languages mostly used in the books\n",
        "     (Ποιες είναι οι πιο συχνές γλώσσες συγγραφής των βιβλίων + φθίνουσα κατάταξη αυτών)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z78UjXyzqO5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "df_2_8 = df.copy()\n",
        "\n",
        "# Remove rows with NaN values in the columns that we are going to process in the sub-question\n",
        "df_2_8.dropna(subset=['title', 'language'], inplace=True)\n",
        "\n",
        "# Count the number of books for each language\n",
        "language_counts = df_2_8['language'].value_counts()\n",
        "\n",
        "# Get the unique languages and their corresponding counts\n",
        "languages = language_counts.index\n",
        "counts = language_counts.values\n",
        "\n",
        "# Generate random colors with number equal to the number of languages existing in the dataframe\n",
        "random_colors = ['#%06x' % random.randint(0, 0xFFFFFF) for _ in range(len(languages))]\n",
        "\n",
        "# Plot the color plot with different colors for each bar, to better visualise data\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Adjust the width of the bars dynamically based on the number of languages and add padding on both sides\n",
        "bar_width = 1 / (len(languages) + 2)\n",
        "\n",
        "bars = plt.bar(range(len(languages)), counts, width=0.6)\n",
        "\n",
        "# Assign different colors to each bar\n",
        "for i, bar in enumerate(bars):\n",
        "    bar.set_color(random_colors[i])\n",
        "\n",
        "# Set y-axis scale to logarithmic for better visualization of small and very large numbers\n",
        "plt.yscale('log')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Number of Books per Language')\n",
        "plt.xlabel('Language')\n",
        "plt.ylabel('Number of Books')\n",
        "\n",
        "# Rotate and align x-axis labels\n",
        "plt.xticks(range(len(languages)), languages, rotation=90, ha='center', fontsize=8)\n",
        "\n",
        "# Add the count above each bar\n",
        "for i, count in enumerate(counts):\n",
        "    plt.text(i, count, count, ha='center', va='bottom', fontsize=8, rotation=90)\n",
        "\n",
        "# Plotting\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgDeFL8cdhG_"
      },
      "source": [
        "9.   Find the 10 publishers with the most publications\n",
        "     (Ποιοι είναι οι 10 εκδότες με τις περισσότερες εκδόσεις βιβλίων)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyzVv567yhbY"
      },
      "outputs": [],
      "source": [
        "df_2_9 = df.copy()\n",
        "\n",
        "# Remove rows with NaN values in the columns that we are going to process in the sub-question\n",
        "df_2_9.dropna(subset=['publisher'], inplace=True)\n",
        "\n",
        "# Group the dataframe by publisher and count the publications\n",
        "publisher_publications = df_2_9['publisher'].value_counts().nlargest(10)\n",
        "\n",
        "# Generate a color palette with a unique color for each publisher\n",
        "colors = plt.cm.tab10(np.arange(len(publisher_publications)))\n",
        "\n",
        "# Plot the histogram with colored bars\n",
        "plt.figure(figsize=(10, 10))\n",
        "publisher_publications.plot(kind='bar', color=colors)\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Top 10 Publishers with Most Publications')\n",
        "plt.xlabel('Publisher')\n",
        "plt.ylabel('Number of Publications')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=8)     # Rotate and align x-axis labels\n",
        "plt.ticklabel_format(style='plain', axis='y')       # Display y-axis values in plain format\n",
        "\n",
        "# Add the number of publications on top of each bar\n",
        "for i, count in enumerate(publisher_publications):\n",
        "    plt.text(i, count, count, ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# Plotting\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# free memory\n",
        "del df_2_2, df_2_5, df_2_6, df_2_8, df_2_9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osdwZbxAtMFA"
      },
      "source": [
        "### Question 3: Recommendation System (RS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94x92azkgLCp"
      },
      "outputs": [],
      "source": [
        "import random # import again for code execution independence\n",
        "\n",
        "# Creating the new DataFrame df_RS, by keeping only the 'bookId'\n",
        "# and 'description' columns as well as the rows where 'language'=\"English\"\n",
        "\n",
        "df_RS_all = df.loc[df['language'] == 'English', ['bookId', 'description']]\n",
        "\n",
        "# Print the first 50 rows of the resulting DataFrame, as an example, to avoid RAM crash\n",
        "df_RS_all.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSbjvz4OlPoX"
      },
      "source": [
        "The TF-IDF matrix (Term Frequency-Inverse Document Frequency) is a mathematical/numeric representation of text that provides information about the rarity of words and structures within the broader context of the text.\n",
        "\n",
        "* Term Frequency (TF): It measures the frequency of occurrence of a term (e.g., words) within the text. When the TF of a term is high, it means that the term appears more frequently in the text.\n",
        "\n",
        "* Inverse Document Frequency (IDF): It is a measure of the rarity of a term, not in a single document, but in a collection/group of texts (e.g., 100 books in a library). The higher the IDF value of a term, the rarer it is within the collection of texts, and therefore, it gains more importance in various data mining processes since its occurrence is considered \"rare\" and each finding carries greater \"significance.\"\n",
        "\n",
        "The TF-IDF score of each term is equal to the product of these two values (TF * IDF). The TF-IDF matrix consists of rows corresponding to different texts/documents and columns corresponding to different terms that appear in the collection (in some or all of the documents)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU0oMQcaoaRh"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Remove rows with NaN values in the 'description' column\n",
        "df_RS_all.dropna(subset=['description'], inplace=True)\n",
        "\n",
        "# Reduce the dataset to avoid RAM crash\n",
        "df_RS = df_RS_all.head(15000)\n",
        "\n",
        "# Extract the 'description' column from the DataFrame\n",
        "descriptions = df_RS['description'].tolist()\n",
        "\n",
        "# UNIGRAMS -> Create an instance of TfidfVectorizer with ngram_range=(1, 1) and default stop words for the English language\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
        "tfidf_matrix_unigrams = vectorizer.fit_transform(descriptions)\n",
        "\n",
        "# BIGRAMS -> Create an instance of TfidfVectorizer with ngram_range=(2, 2) and default stop words for the English language\n",
        "vectorizer = TfidfVectorizer(ngram_range=(2, 2), stop_words='english')\n",
        "tfidf_matrix_bigrams = vectorizer.fit_transform(descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4hsHPSPRjb6"
      },
      "source": [
        "* UNIGRAMS 100 most similar books"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWg-tZWhOCUN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "titles = df['title'].tolist()   # we want this to print the 'title' of the book\n",
        "\n",
        "# Calculate cosine similarity on the UNIGRAMS TF-IDF matrix\n",
        "cosine_sim_unigrams = cosine_similarity(tfidf_matrix_unigrams, tfidf_matrix_unigrams)\n",
        "\n",
        "print(\"> UNIGRAMS: 100 most similar books\")\n",
        "print(\"------------------------------------\\n\")\n",
        "\n",
        "# Get the indices of the top 100 books after sorting in descending order\n",
        "similar_books_indices_unigrams = np.argsort(-cosine_sim_unigrams)[:,:101]\n",
        "\n",
        "# Get the indices of the top 100 similar books excluding the self-similar ones (diagonal)\n",
        "similar_books_indices_unigrams = similar_books_indices_unigrams[:, 1:]\n",
        "\n",
        "# Flatten the indices array to process them next\n",
        "similar_books_indices_unigrams = similar_books_indices_unigrams.flatten()\n",
        "\n",
        "# Get unique indices and keep only the first 100 indices\n",
        "similar_books_indices_unigrams = np.unique(similar_books_indices_unigrams)[:100]\n",
        "\n",
        "# Create a dictionary to store the most similar books\n",
        "similar_books_uni_dict = {}\n",
        "\n",
        "# Print the most similar books using unigrams\n",
        "for book_index in similar_books_indices_unigrams:\n",
        "    similar_books_uni_dict[book_index] = titles[book_index]\n",
        "    print(titles[book_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVhD-B-xRn1G"
      },
      "source": [
        "* BIGRAMS 100 most similar books"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07FaOo7_ROji"
      },
      "outputs": [],
      "source": [
        "# Calculate cosine similarity on the BIGRAMS TF-IDF matrix\n",
        "cosine_sim_bigrams = cosine_similarity(tfidf_matrix_bigrams, tfidf_matrix_bigrams)\n",
        "\n",
        "print(\"> BIGRAMS: 100 most similar books\")\n",
        "print(\"------------------------------------\\n\")\n",
        "\n",
        "# Get the indices of the top 100 books after sorting in descending order\n",
        "similar_books_indices_bigrams = np.argsort(-cosine_sim_bigrams)[:,:101]\n",
        "\n",
        "# Get the indices of the top 100 similar books excluding the self-similar ones (diagonal)\n",
        "similar_books_indices_bigrams = similar_books_indices_bigrams[:, 1:]\n",
        "\n",
        "# Flatten the indices array to process them next\n",
        "similar_books_indices_bigrams = similar_books_indices_bigrams.flatten()\n",
        "\n",
        "# Get unique indices and keep only the first 100 indices\n",
        "similar_books_indices_bigrams = np.unique(similar_books_indices_bigrams)[:100]\n",
        "\n",
        "# Create a dictionary to store the most similar books\n",
        "similar_books_bi_dict = {}\n",
        "\n",
        "# Print the most similar books using unigrams\n",
        "for book_index in similar_books_indices_bigrams:\n",
        "    similar_books_bi_dict[book_index] = titles[book_index]\n",
        "    print(titles[book_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfoCVIxtMPk"
      },
      "source": [
        "Unigrams and bigrams are levels of tokenization in language text processing that can be used to draw conclusions, such as linguistic associations or the recognition of lexical patterns.\n",
        "\n",
        "Unigrams refer to the subdivision of the text into individual tokens (words). For example, the sentence \"A comet fell to Earth\" consists of 5 unigrams: \"A\", \"comet\", \"fell\", \"to\", \"Earth\".\n",
        "\n",
        "Bigrams, on the other hand, involve dividing a text into tokens that consist of pairs of consecutive words. In the previous example, the bigrams would be: \"A comet\", \"comet fell\", \"fell to\", \"to Earth\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQQff0OBobOQ"
      },
      "source": [
        "* Functions for computing the N most similar books to a specific book using UNIGRAMS and BIGRAMS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzE_QxWlV8QO"
      },
      "outputs": [],
      "source": [
        "# function to find the N-most similar books, when a specific book is given as input, using UNIGRAMS\n",
        "def unigrams_recommend(bookId, N):\n",
        "\n",
        "    # beacuse 1 of the N boos is the book itself, in the 'for' loop we ignore it\n",
        "    # so we need to search for 1 more\n",
        "    N += 1\n",
        "\n",
        "    # Get the index of the book in the DataFrame\n",
        "    book_index = df_RS.loc[df['bookId'] == bookId].index[0]\n",
        "\n",
        "    # Create a TF-IDF matrix for unigrams\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix_unigrams = vectorizer.fit_transform(df_RS['description'])\n",
        "\n",
        "    # Calculate cosine similarity on the unigrams TF-IDF matrix\n",
        "    cosine_sim_unigrams = cosine_similarity(tfidf_matrix_unigrams, tfidf_matrix_unigrams)\n",
        "\n",
        "    # Get the indices and similarity scores of the most similar books\n",
        "    similar_books_indices = np.argsort(-cosine_sim_unigrams[book_index])[:N]\n",
        "    similarity_scores = cosine_sim_unigrams[book_index][similar_books_indices]\n",
        "\n",
        "    # Print the recommendation results\n",
        "    blank = \"\"\n",
        "    print(f\"Recommending {N-1} books similar to: ' {book_title} '\\n\")\n",
        "\n",
        "    for i in range(N):\n",
        "        recommended_book_index = similar_books_indices[i]\n",
        "\n",
        "        # Skip the self-similar book\n",
        "        if recommended_book_index == book_index:\n",
        "            continue\n",
        "\n",
        "        recommended_book_title = df.loc[recommended_book_index, 'title']\n",
        "        recommended_book_description = df.loc[recommended_book_index, 'description']\n",
        "        similarity_score = similarity_scores[i]\n",
        "        print(f\"{i}. Recommended: {recommended_book_title}\")\n",
        "        print(f\"   Description: {recommended_book_description}\")\n",
        "        print(f\"   (score: {similarity_score})\\n\")\n",
        "\n",
        "    print(\"---------------------------------------------------------\\n\")\n",
        "\n",
        "\n",
        "# test UNIGRAM recommending function\n",
        "print(\"----------------------- UNIGRAMS recommendation function -----------------------\\n\")\n",
        "random_bookIds = df_RS['bookId'].sample(n=5, random_state=42).tolist()\n",
        "\n",
        "# Testing the recommend_similar_books function for each random bookId\n",
        "count = 1\n",
        "for bookId in random_bookIds:\n",
        "    # Find the title of the book with the given bookId in the df DataFrame\n",
        "    book_title = df.loc[df['bookId'] == bookId, 'title'].values[0]\n",
        "    print(f\"> ({count})\")\n",
        "    print(f\"Random book selection: ' {book_title} '\")\n",
        "\n",
        "    # recommend N-random similar books for a random book\n",
        "    unigrams_recommend(bookId, random.randint(3, 10))\n",
        "    count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XlIztnkXFnI"
      },
      "outputs": [],
      "source": [
        "# function to find the N-most similar books, when a specific book is given as input, using BIGRAMS\n",
        "def bigrams_recommend(bookId, N):\n",
        "\n",
        "    # beacuse 1 of the N boos is the book itself, in the 'for' loop we ignore it\n",
        "    # so we need to search for 1 more\n",
        "    N += 1\n",
        "\n",
        "    # Get the index of the book in the DataFrame\n",
        "    book_index = df_RS.loc[df['bookId'] == bookId].index[0]\n",
        "\n",
        "    # Create a TF-IDF matrix for unigrams\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix_bigrams = vectorizer.fit_transform(df_RS['description'])\n",
        "\n",
        "    # Calculate cosine similarity on the unigrams TF-IDF matrix\n",
        "    cosine_sim_bigrams = cosine_similarity(tfidf_matrix_bigrams, tfidf_matrix_bigrams)\n",
        "\n",
        "    # Get the indices and similarity scores of the most similar books\n",
        "    similar_books_indices = np.argsort(-cosine_sim_bigrams[book_index])[:N]\n",
        "    similarity_scores = cosine_sim_bigrams[book_index][similar_books_indices]\n",
        "\n",
        "    # Print the recommendation results\n",
        "    print(f\"   Recommending {N-1} books similar to: ' {book_title} '\\n\")\n",
        "\n",
        "    for i in range(N):\n",
        "        recommended_book_index = similar_books_indices[i]\n",
        "\n",
        "        # Skip the self-similar book\n",
        "        if recommended_book_index == book_index:\n",
        "            continue\n",
        "\n",
        "        recommended_book_title = df.loc[recommended_book_index, 'title']\n",
        "        recommended_book_description = df.loc[recommended_book_index, 'description']\n",
        "        similarity_score = similarity_scores[i]\n",
        "        print(f\"{i}. Recommended: {recommended_book_title}\")\n",
        "        print(f\"   Description: {recommended_book_description}\")\n",
        "        print(f\"   (score: {similarity_score})\\n\")\n",
        "\n",
        "    print(\"---------------------------------------------------------\\n\")\n",
        "\n",
        "\n",
        "# test BIGRAM recommending function\n",
        "print(\"----------------------- BIGRAMS recommendation function -----------------------\\n\")\n",
        "random_bookIds = df_RS['bookId'].sample(n=5, random_state=30).tolist()\n",
        "\n",
        "# Testing the recommend_similar_books function for each random bookId\n",
        "count = 1\n",
        "for bookId in random_bookIds:\n",
        "    # Find the title of the book with the given bookId in the df DataFrame\n",
        "    book_title = df.loc[df['bookId'] == bookId, 'title'].values[0]\n",
        "    print(f\"> ({count})\")\n",
        "    print(f\"Random book selection: ' {book_title} '\")\n",
        "\n",
        "    # recommend N-random similar books for a random book\n",
        "    unigrams_recommend(bookId, random.randint(3, 10))\n",
        "    count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTMehdkVobQ9"
      },
      "source": [
        "### Question 4: Classification\n",
        "First, data preprocessing is performed for clustering. Specifically, a new dataframe is created with the columns \"*bookId*\", \"*description*\", and \"*genreSingle*\" that contains the rows from the original dataframe with genres from the top 10. Then, the characters in the \"*description*\" are converted to lowercase, and punctuation marks and leading/trailing whitespace are removed from the \"*description*\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eov5PPWXFB8z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "# Download most popular nltk packages\n",
        "nltk.download('popular', quiet=True)\n",
        "\n",
        "# Use df2 since it contains genreSingle\n",
        "# Remove rows that contain the `]` character\n",
        "df2 = df2[~df2['genreSingle'].str.contains(']')]\n",
        "\n",
        "# Find the top 10 genres\n",
        "top_10_genres = df2['genreSingle'].value_counts().nlargest(10).index.tolist()\n",
        "\n",
        "# Define the meanings for each category\n",
        "genre_meanings = {\n",
        "    genre: genre for genre in top_10_genres\n",
        "}\n",
        "\n",
        "# Create a new dataframe that contains the most 10 book genres\n",
        "clf_df = df2[df2['genreSingle'].isin(top_10_genres)].copy()\n",
        "\n",
        "# Reduce amount of data\n",
        "clf_df = clf_df.head(15000)\n",
        "\n",
        "# Keep only bookId, description and genreSingle\n",
        "clf_df = clf_df[['bookId', 'description', 'genreSingle']]\n",
        "\n",
        "# Clean data of description column\n",
        "def clean_description(description):\n",
        "    description = re.sub(r'\\W', ' ', str(description))    # remove all special characters\n",
        "    description = description.lower()                     # convert description to lowercase\n",
        "    description = re.sub(r'\\s+', ' ', description)        # remove extra spaces\n",
        "    return description\n",
        "\n",
        "clf_df['description'] = clf_df['description'].apply(clean_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPyg7wImobT5"
      },
      "source": [
        "Then, the descriptions are converted into lists of words, with which we create a word2vec model. This model contains 300-dimensional vectors for each description, and we find the average of these vectors for each description. Finally, we use the pickle library to store the results in pkl files, so that if they already exist, the computations won't be repeated. This means that for any data updates, the .pkl files will need to be deleted first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmgtCepNFJoU"
      },
      "outputs": [],
      "source": [
        "# Tokenize descriptions with word_tokenize function\n",
        "clf_df['description'] = clf_df['description'].apply(lambda x: nltk.tokenize.word_tokenize(x))\n",
        "\n",
        "\n",
        "# Check if the model is already trained and saved\n",
        "if os.path.exists('w2v_model.pkl'):\n",
        "    with open('w2v_model.pkl', 'rb') as file:\n",
        "        w2v_model = pickle.load(file)\n",
        "\n",
        "else:\n",
        "    # Train the Word2Vec model\n",
        "    w2v_model = Word2Vec(clf_df['description'], vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Save the model for later use\n",
        "    with open('w2v_model.pkl', 'wb') as file:\n",
        "        pickle.dump(w2v_model, file)\n",
        "\n",
        "# Calculate the average feature vector for each description\n",
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    # Create an array of zeroes\n",
        "    feature_vector = np.zeros((num_features, ), dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "\n",
        "    # Sum and number of words\n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model.wv[word])\n",
        "\n",
        "    # Find and return the average\n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "# Check if the feature vectors are already calculated and saved\n",
        "if os.path.exists('description_features.pkl'):\n",
        "    with open('description_features.pkl', 'rb') as file:\n",
        "        description_features = pickle.load(file)\n",
        "\n",
        "else:\n",
        "    # Convert list of words from w2v model to a set for better perfomance\n",
        "    vocabulary = set(w2v_model.wv.index_to_key)\n",
        "\n",
        "    # Calculate the average for each tokenized description\n",
        "    features = [average_word_vectors(tokenized_sentence, w2v_model, vocabulary, 300)\n",
        "                    for tokenized_sentence in clf_df['description']]\n",
        "\n",
        "    # Convert np array to Dataframe\n",
        "    description_features = pd.DataFrame(features)\n",
        "\n",
        "    # Save the feature vectors for later use\n",
        "    with open('description_features.pkl', 'wb') as file:\n",
        "        pickle.dump(description_features, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COM1CFRtobXD"
      },
      "source": [
        "To split the data into train and test sets, we first need to convert the categorical values in the \"*genreSingle*\" column into numerical values. Then, we execute three different classification methods in the following order: Naive Bayes, SVM, and Random Forests. Specifically, for the SVM method, the GridSearchCV() function is used to select the appropriate combination of parameters from the initial set of parameters provided. After the calculations are completed, the results of the methods are printed, showing the precision, recall, and f1-score achieved by the models for each \"*genre*\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo49kIgOFNYB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "\n",
        "# Print the category and its meaning for better reference in the following printings\n",
        "i = 0\n",
        "for category in top_10_genres:\n",
        "    meaning = genre_meanings.get(category, 'Unknown')\n",
        "    print(f\"Category {i}: {meaning}\")\n",
        "    i += 1\n",
        "\n",
        "# Create a label encoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to the 'genreSingle' column\n",
        "clf_df['genreSingle_encoded'] = le.fit_transform(clf_df['genreSingle'])\n",
        "\n",
        "# Drop the original 'genreSingle' column\n",
        "clf_df = clf_df.drop('genreSingle', axis=1)\n",
        "\n",
        "# Split train and test sets (80-20) from genreSingle_encoded data\n",
        "x = description_features            # all columns except genreSingle_encoded\n",
        "y = clf_df['genreSingle_encoded']   # the newly created numeric genreSingle_encoded column\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(x_train, y_train)\n",
        "nb_predictions = nb.predict(x_test)\n",
        "\n",
        "# Support Vector Machines\n",
        "parameters = {'kernel': ['linear', 'rbf'], 'gamma': [0.01, 0.1, 1, 10], 'C': [0.01, 0.1, 1, 10]}\n",
        "svc = SVC()\n",
        "\n",
        "# Use GridSearchCV to decide which parameters perform better\n",
        "clf = GridSearchCV(svc, parameters, cv=10)\n",
        "clf.fit(x_train, y_train)\n",
        "svm_predictions = clf.predict(x_test)\n",
        "\n",
        "# Random Forests\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(x_train, y_train)\n",
        "rf_predictions = rf.predict(x_test)\n",
        "\n",
        "# Naive Bayes evaluation\n",
        "print(\"\\n----------------------------Naive Bayes evaluation----------------------------\\n\")\n",
        "nb_report = classification_report(y_test, nb_predictions)\n",
        "print(nb_report)\n",
        "\n",
        "# SVM evaluation\n",
        "print(\"\\n----------------------------SVM evaluation----------------------------\\n\")\n",
        "svm_report = classification_report(y_test, svm_predictions)\n",
        "print(svm_report)\n",
        "\n",
        "# Random Forest evaluation\n",
        "print(\"\\n----------------------------Random Forest evaluation----------------------------\\n\")\n",
        "rf_report = classification_report(y_test, rf_predictions)\n",
        "print(rf_report)\n",
        "\n",
        "# 10-fold Cross Validation for each model\n",
        "models = [nb, clf, rf]\n",
        "model_names = ['Naive Bayes', 'SVM', 'Random Forest']\n",
        "\n",
        "# 10-fold Cross Validation for each model\n",
        "for i, model in enumerate(models):\n",
        "    print(\"\\n---------------------10-fold Cross Validation for\", model_names[i], \"---------------------\\n\")\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "        scores_precision = cross_val_score(model, description_features, clf_df['genreSingle_encoded'], cv=10, scoring='precision_macro')\n",
        "        print(\"Precision: %0.2f (+/- %0.2f)\" % (scores_precision.mean(), scores_precision.std() * 2))\n",
        "\n",
        "        scores_recall = cross_val_score(model, description_features, clf_df['genreSingle_encoded'], cv=10, scoring='recall_macro')\n",
        "        print(\"Recall: %0.2f (+/- %0.2f)\" % (scores_recall.mean(), scores_recall.std() * 2))\n",
        "\n",
        "        scores_f1 = cross_val_score(model, description_features, clf_df['genreSingle_encoded'], cv=10, scoring='f1_macro')\n",
        "        print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores_f1.mean(), scores_f1.std() * 2))\n",
        "\n",
        "        scores_accuracy = cross_val_score(model, description_features, clf_df['genreSingle_encoded'], cv=10, scoring='accuracy')  #, error_score='raise'\n",
        "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_accuracy.mean(), scores_accuracy.std() * 2))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}